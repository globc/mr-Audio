 # Copyright (c) 2022, salesforce.com, inc.
 # All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause

model:
  arch: blip2_mr_audio_xinstructblip
  model_type: pretrain_flant5xl
  load_finetuned: False # True
  use_grad_checkpoint: False
  vit_precision: fp16
  beats_precision: fp16
  freeze_vit: True
  freeze_beats: True
  audio_encoder_kwargs :  {"checkpoint_path": "/home/atuin/g102ea/shared/group_2/checkpoints/X-InstructBLIP/BEATs_iter3_plus_AS2M.pt"}
  pretrained_audio_qformer: https://storage.googleapis.com/sfr-xinstructblip-data-research/model/xinstructblip_checkpoints/vicuna7b/audio_qformer_improved.pth
  task: qformer_freeze_lora
  input_time_format: seconds_integers # [seconds_integers | seconds_floats | relative_integers | relative_floats | framenumbers | False]
  interleave_data: True
  frame_token_aggregation: False # [mean | False]

datasets:
  qvh_audio: # name of the dataset builder
    data_type: [video, audio]

    video_processor:
        train:
          name: "blip2_video_train"
          n_frms: 20
          image_size: 224
        eval:
          name: "blip_video_eval"
          n_frms: 20
          image_size: 224

    audio_processor:
        train:
          name: beats_audio
          sampling_rate: 16000
          n_frames: 20
          frame_length: 256
        eval:
          name: beats_audio
          sampling_rate: 16000
          n_frames: 20
          is_eval: False
          frame_length: 256

    text_processor:
          train:
            name: "blip_question"
            max_words: 50
          eval:
            name: "blip_question"
            max_words: 50

    build_info:
      # Be careful not to append minus sign (-) before split to avoid itemizing
      annotations:
        train:
          url:
            - /home/atuin/g102ea/g102ea13/mr-Audio/mr_BLIP_data/charades_sta_annotations/lavis/new_train.json
          storage:
            - /home/atuin/g102ea/g102ea13/mr-Audio/mr_BLIP_data/charades_sta_annotations/lavis/new_train.json
        val:
          url:
            - /home/atuin/g102ea/g102ea13/mr-Audio/mr_BLIP_data/charades_sta_annotations/lavis/new_val.json
          storage:
            - /home/atuin/g102ea/g102ea13/mr-Audio/mr_BLIP_data/charades_sta_annotations/lavis/new_val.json
        test:
          # url: Your/path/to/test_dummy.json
          # storage: Your/path/to/test_dummy.json
          url:
            - /home/atuin/g102ea/g102ea13/mr-Audio/mr_BLIP_data/charades_sta_annotations/lavis/test.json
          storage:
            - /home/atuin/g102ea/g102ea13/mr-Audio/mr_BLIP_data/charades_sta_annotations/lavis/test.json
      video:
        storage: /home/atuin/g102ea/shared/videos/Charades_v1
      audio:
        storage: /home/atuin/g102ea/shared/videos/Charades_v1

run:
  task: moment_retrieval
  # optimizer
  lr_sched: "linear_warmup_cosine_lr"
  init_lr: 3e-4 # vid2seq: 3e-4 | before: 5e-5
  min_lr: 0
  warmup_lr: 1e-8
  warmup_steps: 1745 # 10% * 50 epochs * 349 steps/epoch = 1745
  weight_decay: 0.05
  max_epoch: 50
  batch_size_train: 16
  batch_size_eval: 8
  num_workers: 2
  accum_grad_iters: 1

  max_len: 200
  min_len: 8
  num_beams: 5

  seed: 42
  output_dir: "result/mr_BLIP/Charades/"

  amp: True
  resume_ckpt_path: null

  evaluate: False
  train_splits: ["train"]
  valid_splits: ["val"]
  test_splits: ["test"]

  device: "cuda"
  world_size: 2 # for Debug, original   4
  dist_url: "env://"
  distributed: True
  find_unused_parameters: False

  wandb: True
  wandb_project: "mr_BLIP"
  wandb_name: "Charades_AudioOnly_BEATS_withoutQF"

