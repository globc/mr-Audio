WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/home/hpc/g102ea/g102ea22/.local/lib/python3.9/site-packages/fairscale/experimental/nn/offload.py:19: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  return torch.cuda.amp.custom_fwd(orig_func)  # type: ignore
/home/hpc/g102ea/g102ea22/.local/lib/python3.9/site-packages/fairscale/experimental/nn/offload.py:19: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  return torch.cuda.amp.custom_fwd(orig_func)  # type: ignore
/home/hpc/g102ea/g102ea22/.local/lib/python3.9/site-packages/fairscale/experimental/nn/offload.py:30: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  return torch.cuda.amp.custom_bwd(orig_func)  # type: ignore
/home/hpc/g102ea/g102ea22/.local/lib/python3.9/site-packages/fairscale/experimental/nn/offload.py:30: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  return torch.cuda.amp.custom_bwd(orig_func)  # type: ignore
/home/hpc/g102ea/g102ea22/.local/lib/python3.9/site-packages/fairscale/experimental/nn/offload.py:19: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  return torch.cuda.amp.custom_fwd(orig_func)  # type: ignore
/home/hpc/g102ea/g102ea22/.local/lib/python3.9/site-packages/fairscale/experimental/nn/offload.py:30: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  return torch.cuda.amp.custom_bwd(orig_func)  # type: ignore
/home/hpc/g102ea/g102ea22/.local/lib/python3.9/site-packages/fairscale/experimental/nn/offload.py:19: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  return torch.cuda.amp.custom_fwd(orig_func)  # type: ignore
/home/hpc/g102ea/g102ea22/.local/lib/python3.9/site-packages/fairscale/experimental/nn/offload.py:30: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  return torch.cuda.amp.custom_bwd(orig_func)  # type: ignore
[rank2]:[W1215 16:59:23.737405576 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W1215 16:59:23.742284599 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W1215 16:59:23.751931135 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W1215 16:59:23.753230540 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: ERROR api_key not configured (no-tty). call wandb.login(key=[your_api_key])
Traceback (most recent call last):
  File "/home/atuin/g102ea/g102ea22/mr-Audio/train.py", line 154, in <module>
    main()
  File "/home/atuin/g102ea/g102ea22/mr-Audio/train.py", line 114, in main
    wandb.init(
  File "/home/hpc/g102ea/g102ea22/.local/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 1312, in init
    wandb._sentry.reraise(e)
  File "/home/hpc/g102ea/g102ea22/.local/lib/python3.9/site-packages/wandb/analytics/sentry.py", line 156, in reraise
    raise exc.with_traceback(sys.exc_info()[2])
  File "/home/hpc/g102ea/g102ea22/.local/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 1290, in init
    wi.setup(
  File "/home/hpc/g102ea/g102ea22/.local/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 289, in setup
    wandb_login._login(
  File "/home/hpc/g102ea/g102ea22/.local/lib/python3.9/site-packages/wandb/sdk/wandb_login.py", line 337, in _login
    wlogin.prompt_api_key()
  File "/home/hpc/g102ea/g102ea22/.local/lib/python3.9/site-packages/wandb/sdk/wandb_login.py", line 271, in prompt_api_key
    raise UsageError("api_key not configured (no-tty). call " + directive)
wandb.errors.errors.UsageError: api_key not configured (no-tty). call wandb.login(key=[your_api_key])
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/atuin/g102ea/g102ea22/mr-Audio/train.py", line 154, in <module>
[rank0]:     main()
[rank0]:   File "/home/atuin/g102ea/g102ea22/mr-Audio/train.py", line 114, in main
[rank0]:     wandb.init(
[rank0]:   File "/home/hpc/g102ea/g102ea22/.local/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 1312, in init
[rank0]:     wandb._sentry.reraise(e)
[rank0]:   File "/home/hpc/g102ea/g102ea22/.local/lib/python3.9/site-packages/wandb/analytics/sentry.py", line 156, in reraise
[rank0]:     raise exc.with_traceback(sys.exc_info()[2])
[rank0]:   File "/home/hpc/g102ea/g102ea22/.local/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 1290, in init
[rank0]:     wi.setup(
[rank0]:   File "/home/hpc/g102ea/g102ea22/.local/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 289, in setup
[rank0]:     wandb_login._login(
[rank0]:   File "/home/hpc/g102ea/g102ea22/.local/lib/python3.9/site-packages/wandb/sdk/wandb_login.py", line 337, in _login
[rank0]:     wlogin.prompt_api_key()
[rank0]:   File "/home/hpc/g102ea/g102ea22/.local/lib/python3.9/site-packages/wandb/sdk/wandb_login.py", line 271, in prompt_api_key
[rank0]:     raise UsageError("api_key not configured (no-tty). call " + directive)
[rank0]: wandb.errors.errors.UsageError: api_key not configured (no-tty). call wandb.login(key=[your_api_key])
[rank0]:[W1215 16:59:27.500199964 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W1215 16:59:28.986153 436150 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 436225 closing signal SIGTERM
W1215 16:59:28.987806 436150 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 436226 closing signal SIGTERM
W1215 16:59:28.988626 436150 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 436227 closing signal SIGTERM
E1215 16:59:29.531913 436150 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 436224) of binary: /apps/python/3.9-anaconda/bin/python
Traceback (most recent call last):
  File "/apps/python/3.9-anaconda/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/apps/python/3.9-anaconda/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/hpc/g102ea/g102ea22/.local/lib/python3.9/site-packages/torch/distributed/run.py", line 923, in <module>
    main()
  File "/home/hpc/g102ea/g102ea22/.local/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/home/hpc/g102ea/g102ea22/.local/lib/python3.9/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/home/hpc/g102ea/g102ea22/.local/lib/python3.9/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/home/hpc/g102ea/g102ea22/.local/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/hpc/g102ea/g102ea22/.local/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-12-15_16:59:28
  host      : a0901.nhr.fau.de
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 436224)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
