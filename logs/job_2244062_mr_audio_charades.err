WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/fairscale/experimental/nn/offload.py:19: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  return torch.cuda.amp.custom_fwd(orig_func)  # type: ignore
/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/fairscale/experimental/nn/offload.py:19: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  return torch.cuda.amp.custom_fwd(orig_func)  # type: ignore
/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/fairscale/experimental/nn/offload.py:30: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  return torch.cuda.amp.custom_bwd(orig_func)  # type: ignore
/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/fairscale/experimental/nn/offload.py:30: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  return torch.cuda.amp.custom_bwd(orig_func)  # type: ignore
[rank0]:[W1216 03:12:58.535897907 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/fairscale/experimental/nn/offload.py:19: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  return torch.cuda.amp.custom_fwd(orig_func)  # type: ignore
/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/fairscale/experimental/nn/offload.py:30: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  return torch.cuda.amp.custom_bwd(orig_func)  # type: ignore
/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/fairscale/experimental/nn/offload.py:19: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  return torch.cuda.amp.custom_fwd(orig_func)  # type: ignore
/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/fairscale/experimental/nn/offload.py:30: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  return torch.cuda.amp.custom_bwd(orig_func)  # type: ignore
[rank2]:[W1216 03:12:58.557844520 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W1216 03:13:00.084541097 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W1216 03:13:00.086375944 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: joeltschesche (joeltschesche-tu-darmstadt). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/atuin/g102ea/g102ea22/mr-Audio/wandb/run-20241216_031302-reh4suke
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Charades_20
wandb: ‚≠êÔ∏è View project at https://wandb.ai/joeltschesche-tu-darmstadt/mr_BLIP
wandb: üöÄ View run at https://wandb.ai/joeltschesche-tu-darmstadt/mr_BLIP/runs/reh4suke
2024-12-16 03:13:03,466 [INFO] 
=====  Running Parameters    =====
2024-12-16 03:13:03,466 [INFO] {
    "accum_grad_iters": 2,
    "amp": true,
    "batch_size_eval": 4,
    "batch_size_train": 4,
    "device": "cuda",
    "dist_backend": "nccl",
    "dist_url": "env://",
    "distributed": true,
    "evaluate": false,
    "find_unused_parameters": true,
    "gpu": 0,
    "init_lr": 0.0003,
    "lr_sched": "linear_warmup_cosine_lr",
    "max_epoch": 50,
    "max_len": 200,
    "min_len": 8,
    "min_lr": 0,
    "num_beams": 5,
    "num_workers": 8,
    "output_dir": "result/mr_BLIP/Charades/Charades_20-21",
    "rank": 0,
    "resume_ckpt_path": null,
    "seed": 42,
    "task": "moment_retrieval",
    "test_splits": [
        "test"
    ],
    "train_splits": [
        "train"
    ],
    "valid_splits": [
        "val"
    ],
    "wandb": true,
    "wandb_name": "Charades_20",
    "wandb_project": "mr_BLIP",
    "warmup_lr": 1e-08,
    "warmup_steps": 1745,
    "weight_decay": 0.05,
    "world_size": 4
}
2024-12-16 03:13:03,466 [INFO] 
======  Dataset Attributes  ======
2024-12-16 03:13:03,467 [INFO] 
======== charades_sta =======
2024-12-16 03:13:03,467 [INFO] {
    "build_info": {
        "annotations": {
            "test": {
                "storage": "/home/atuin/g102ea/shared/charades_sta_annotations/test.json",
                "url": "/home/atuin/g102ea/shared/charades_sta_annotations/test.json"
            },
            "train": {
                "storage": "/home/atuin/g102ea/shared/charades_sta_annotations/train.json",
                "url": "/home/atuin/g102ea/shared/charades_sta_annotations/train.json"
            },
            "val": {
                "storage": "/home/atuin/g102ea/shared/charades_sta_annotations/test.json",
                "url": "/home/atuin/g102ea/shared/charades_sta_annotations/test.json"
            }
        },
        "videos": {
            "storage": "/home/atuin/g102ea/shared/videos/Charades_v1"
        }
    },
    "data_type": "videos",
    "text_processor": {
        "eval": {
            "max_words": 50,
            "name": "blip_question"
        },
        "train": {
            "max_words": 50,
            "name": "blip_question"
        }
    },
    "vis_processor": {
        "eval": {
            "image_size": 224,
            "n_frms": 20,
            "name": "blip_video_eval"
        },
        "train": {
            "image_size": 224,
            "n_frms": 20,
            "name": "blip2_video_train"
        }
    }
}
2024-12-16 03:13:03,467 [INFO] 
======  Model Attributes  ======
2024-12-16 03:13:03,467 [INFO] {
    "arch": "blip2_mr",
    "drop_path_rate": 0,
    "finetuned": "",
    "frame_token_aggregation": false,
    "freeze_vit": true,
    "image_size": 224,
    "input_time_format": "seconds_integers",
    "interleave_data": true,
    "load_finetuned": false,
    "model_type": "pretrain_flant5xl",
    "num_query_token": 32,
    "pretrained": "https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained_flant5xl.pth",
    "prompt": "",
    "t5_model": "google/flan-t5-xl",
    "task": "qformer_freeze_lora",
    "use_grad_checkpoint": false,
    "vit_precision": "fp16"
}
2024-12-16 03:13:03,470 [INFO] Using existing file /home/atuin/g102ea/shared/charades_sta_annotations/train.json.
2024-12-16 03:13:03,470 [INFO] Using existing file /home/atuin/g102ea/shared/charades_sta_annotations/test.json.
2024-12-16 03:13:03,471 [INFO] Using existing file /home/atuin/g102ea/shared/charades_sta_annotations/test.json.
2024-12-16 03:13:03,471 [INFO] Building datasets...
2024-12-16 03:13:17,023 [WARNING] /home/atuin/g102ea/g102ea22/mr-Audio/lavis/models/eva_vit.py:433: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state_dict = torch.load(cached_file, map_location="cpu")

2024-12-16 03:13:17,023 [WARNING] /home/atuin/g102ea/g102ea22/mr-Audio/lavis/models/eva_vit.py:433: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state_dict = torch.load(cached_file, map_location="cpu")

2024-12-16 03:13:17,023 [WARNING] /home/atuin/g102ea/g102ea22/mr-Audio/lavis/models/eva_vit.py:433: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state_dict = torch.load(cached_file, map_location="cpu")

2024-12-16 03:13:17,023 [WARNING] /home/atuin/g102ea/g102ea22/mr-Audio/lavis/models/eva_vit.py:433: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state_dict = torch.load(cached_file, map_location="cpu")

2024-12-16 03:13:19,811 [INFO] freeze vision encoder
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00,  2.01it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00,  1.93it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  2.81it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  2.63it/s]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  2.84it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  2.67it/s]
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00,  3.01it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00,  3.04it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  3.91it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  3.74it/s]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  3.95it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  3.78it/s]
2024-12-16 03:13:48,656 [INFO] Annoying numbers and their replacement: {112: 113, 128: 129, 135: 136, 161: 160, 162: 164, 163: 164, 170: 169, 171: 172, 173: 174, 175: 176, 181: 180, 182: 183, 191: 192, 193: 192, 194: 195}
2024-12-16 03:13:50,802 [WARNING] /home/atuin/g102ea/g102ea22/mr-Audio/lavis/models/blip2_models/blip2.py:80: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(cached_file, map_location="cpu")

2024-12-16 03:13:50,802 [WARNING] /home/atuin/g102ea/g102ea22/mr-Audio/lavis/models/blip2_models/blip2.py:80: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(cached_file, map_location="cpu")

2024-12-16 03:13:50,802 [WARNING] /home/atuin/g102ea/g102ea22/mr-Audio/lavis/models/blip2_models/blip2.py:80: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(cached_file, map_location="cpu")

2024-12-16 03:13:50,803 [WARNING] /home/atuin/g102ea/g102ea22/mr-Audio/lavis/models/blip2_models/blip2.py:80: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(cached_file, map_location="cpu")

2024-12-16 03:13:51,462 [INFO] load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained_flant5xl.pth
2024-12-16 03:13:51,463 [INFO] load pretrained weights from https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained_flant5xl.pth
2024-12-16 03:13:51,804 [INFO] Start training
2024-12-16 03:13:52,091 [INFO] dataset_ratios not specified, datasets will be concatenated (map-style datasets) or chained (webdataset.DataPipeline).
2024-12-16 03:13:52,091 [INFO] Loaded 12408 records for train split from the dataset.
2024-12-16 03:13:52,091 [INFO] Loaded 3720 records for val split from the dataset.
2024-12-16 03:13:52,092 [INFO] Loaded 3720 records for test split from the dataset.
2024-12-16 03:13:52,125 [INFO] number of trainable parameters: 22825984
2024-12-16 03:13:52,127 [WARNING] /home/atuin/g102ea/g102ea22/mr-Audio/lavis/runners/runner_base.py:147: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self._scaler = torch.cuda.amp.GradScaler()

2024-12-16 03:13:52,127 [WARNING] /home/atuin/g102ea/g102ea22/mr-Audio/lavis/runners/runner_base.py:147: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self._scaler = torch.cuda.amp.GradScaler()

2024-12-16 03:13:52,127 [WARNING] /home/atuin/g102ea/g102ea22/mr-Audio/lavis/runners/runner_base.py:147: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self._scaler = torch.cuda.amp.GradScaler()

2024-12-16 03:13:52,127 [INFO] Start training epoch 0, 775 iters per inner epoch.
2024-12-16 03:13:52,131 [WARNING] /home/atuin/g102ea/g102ea22/mr-Audio/lavis/runners/runner_base.py:147: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self._scaler = torch.cuda.amp.GradScaler()

2024-12-16 03:14:08,031 [WARNING] /home/atuin/g102ea/g102ea22/mr-Audio/lavis/tasks/moment_retrieval.py:241: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):

2024-12-16 03:14:08,593 [WARNING] /home/atuin/g102ea/g102ea22/mr-Audio/lavis/tasks/moment_retrieval.py:241: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):

2024-12-16 03:14:10,402 [WARNING] /home/atuin/g102ea/g102ea22/mr-Audio/lavis/models/blip2_mr_models/blip2_mr.py:1165: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(self.eigendevice != torch.device("cpu"))): #switch to self.device for original

2024-12-16 03:14:10,742 [WARNING] /home/atuin/g102ea/g102ea22/mr-Audio/lavis/tasks/moment_retrieval.py:241: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):

2024-12-16 03:14:10,974 [WARNING] /home/atuin/g102ea/g102ea22/mr-Audio/lavis/models/blip2_mr_models/blip2_mr.py:1165: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(self.eigendevice != torch.device("cpu"))): #switch to self.device for original

2024-12-16 03:14:10,994 [WARNING] /home/atuin/g102ea/g102ea22/mr-Audio/lavis/models/blip2_mr_models/blip2_mr.py:365: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with (torch.cuda.amp.autocast(dtype=torch.float32)):

2024-12-16 03:14:11,481 [WARNING] /home/atuin/g102ea/g102ea22/mr-Audio/lavis/models/blip2_mr_models/blip2_mr.py:365: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with (torch.cuda.amp.autocast(dtype=torch.float32)):

[rank3]: Traceback (most recent call last):
[rank3]:   File "/home/atuin/g102ea/g102ea22/mr-Audio/train.py", line 154, in <module>
[rank3]:     main()
[rank3]:   File "/home/atuin/g102ea/g102ea22/mr-Audio/train.py", line 150, in main
[rank3]:     runner.train()
[rank3]:   File "/home/atuin/g102ea/g102ea22/mr-Audio/lavis/runners/runner_base.py", line 386, in train
[rank3]:     train_stats = self.train_epoch(cur_epoch)
[rank3]:   File "/home/atuin/g102ea/g102ea22/mr-Audio/lavis/runners/runner_base.py", line 445, in train_epoch
[rank3]:     return self.task.train_epoch(
[rank3]:   File "/home/atuin/g102ea/g102ea22/mr-Audio/lavis/tasks/base_task.py", line 116, in train_epoch
[rank3]:     return self._train_inner_loop(
[rank3]:   File "/home/atuin/g102ea/g102ea22/mr-Audio/lavis/tasks/moment_retrieval.py", line 242, in _train_inner_loop
[rank3]:     loss = self.train_step(model=model, samples=samples)
[rank3]:   File "/home/atuin/g102ea/g102ea22/mr-Audio/lavis/tasks/base_task.py", line 67, in train_step
[rank3]:     loss = model(samples)["loss"]
[rank3]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
[rank3]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank3]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
[rank3]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank3]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/home/atuin/g102ea/g102ea22/mr-Audio/lavis/models/blip2_mr_models/blip2_mr.py", line 393, in forward
[rank3]:     outputs_loc = self.t5_model(
[rank3]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/peft/peft_model.py", line 1719, in forward
[rank3]:     return self.base_model(
[rank3]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/peft/tuners/tuners_utils.py", line 197, in forward
[rank3]:     return self.model.forward(*args, **kwargs)
[rank3]:   File "/home/atuin/g102ea/g102ea22/mr-Audio/lavis/models/blip2_models/modeling_t5.py", line 1851, in forward
[rank3]:     encoder_outputs = self.encoder(
[rank3]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/home/atuin/g102ea/g102ea22/mr-Audio/lavis/models/blip2_models/modeling_t5.py", line 1255, in forward
[rank3]:     layer_outputs = layer_module(
[rank3]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/home/atuin/g102ea/g102ea22/mr-Audio/lavis/models/blip2_models/modeling_t5.py", line 788, in forward
[rank3]:     self_attention_outputs = self.layer[0](
[rank3]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/home/atuin/g102ea/g102ea22/mr-Audio/lavis/models/blip2_models/modeling_t5.py", line 675, in forward
[rank3]:     attention_output = self.SelfAttention(
[rank3]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/home/atuin/g102ea/g102ea22/mr-Audio/lavis/models/blip2_models/modeling_t5.py", line 637, in forward
[rank3]:     torch.matmul(attn_weights, value_states)
[rank3]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 3 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 37.70 GiB is allocated by PyTorch, and 298.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank2]: Traceback (most recent call last):
[rank2]:   File "/home/atuin/g102ea/g102ea22/mr-Audio/train.py", line 154, in <module>
[rank2]:     main()
[rank2]:   File "/home/atuin/g102ea/g102ea22/mr-Audio/train.py", line 150, in main
[rank2]:     runner.train()
[rank2]:   File "/home/atuin/g102ea/g102ea22/mr-Audio/lavis/runners/runner_base.py", line 386, in train
[rank2]:     train_stats = self.train_epoch(cur_epoch)
[rank2]:   File "/home/atuin/g102ea/g102ea22/mr-Audio/lavis/runners/runner_base.py", line 445, in train_epoch
[rank2]:     return self.task.train_epoch(
[rank2]:   File "/home/atuin/g102ea/g102ea22/mr-Audio/lavis/tasks/base_task.py", line 116, in train_epoch
[rank2]:     return self._train_inner_loop(
[rank2]:   File "/home/atuin/g102ea/g102ea22/mr-Audio/lavis/tasks/moment_retrieval.py", line 242, in _train_inner_loop
[rank2]:     loss = self.train_step(model=model, samples=samples)
[rank2]:   File "/home/atuin/g102ea/g102ea22/mr-Audio/lavis/tasks/base_task.py", line 67, in train_step
[rank2]:     loss = model(samples)["loss"]
[rank2]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
[rank2]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank2]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
[rank2]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank2]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/home/atuin/g102ea/g102ea22/mr-Audio/lavis/models/blip2_mr_models/blip2_mr.py", line 393, in forward
[rank2]:     outputs_loc = self.t5_model(
[rank2]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/peft/peft_model.py", line 1719, in forward
[rank2]:     return self.base_model(
[rank2]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/peft/tuners/tuners_utils.py", line 197, in forward
[rank2]:     return self.model.forward(*args, **kwargs)
[rank2]:   File "/home/atuin/g102ea/g102ea22/mr-Audio/lavis/models/blip2_models/modeling_t5.py", line 1851, in forward
[rank2]:     encoder_outputs = self.encoder(
[rank2]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/home/atuin/g102ea/g102ea22/mr-Audio/lavis/models/blip2_models/modeling_t5.py", line 1255, in forward
[rank2]:     layer_outputs = layer_module(
[rank2]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/home/atuin/g102ea/g102ea22/mr-Audio/lavis/models/blip2_models/modeling_t5.py", line 788, in forward
[rank2]:     self_attention_outputs = self.layer[0](
[rank2]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/home/atuin/g102ea/g102ea22/mr-Audio/lavis/models/blip2_models/modeling_t5.py", line 675, in forward
[rank2]:     attention_output = self.SelfAttention(
[rank2]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/home/atuin/g102ea/g102ea22/mr-Audio/lavis/models/blip2_models/modeling_t5.py", line 637, in forward
[rank2]:     torch.matmul(attn_weights, value_states)
[rank2]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 2 has a total capacity of 39.50 GiB of which 1.38 MiB is free. Including non-PyTorch memory, this process has 39.49 GiB memory in use. Of the allocated memory 37.53 GiB is allocated by PyTorch, and 334.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2024-12-16 03:14:12,925 [WARNING] /home/atuin/g102ea/g102ea22/mr-Audio/lavis/models/blip2_mr_models/blip2_mr.py:1165: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(self.eigendevice != torch.device("cpu"))): #switch to self.device for original

2024-12-16 03:14:13,433 [WARNING] /home/atuin/g102ea/g102ea22/mr-Audio/lavis/models/blip2_mr_models/blip2_mr.py:365: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with (torch.cuda.amp.autocast(dtype=torch.float32)):

2024-12-16 03:14:14,096 [WARNING] /home/atuin/g102ea/g102ea22/mr-Audio/lavis/tasks/moment_retrieval.py:241: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):

Traceback (most recent call last):
  File "/home/atuin/g102ea/g102ea22/mr-Audio/train.py", line 154, in <module>
    main()
  File "/home/atuin/g102ea/g102ea22/mr-Audio/train.py", line 150, in main
    runner.train()
  File "/home/atuin/g102ea/g102ea22/mr-Audio/lavis/runners/runner_base.py", line 386, in train
    train_stats = self.train_epoch(cur_epoch)
  File "/home/atuin/g102ea/g102ea22/mr-Audio/lavis/runners/runner_base.py", line 445, in train_epoch
    return self.task.train_epoch(
  File "/home/atuin/g102ea/g102ea22/mr-Audio/lavis/tasks/base_task.py", line 116, in train_epoch
    return self._train_inner_loop(
  File "/home/atuin/g102ea/g102ea22/mr-Audio/lavis/tasks/moment_retrieval.py", line 242, in _train_inner_loop
    loss = self.train_step(model=model, samples=samples)
  File "/home/atuin/g102ea/g102ea22/mr-Audio/lavis/tasks/base_task.py", line 67, in train_step
    loss = model(samples)["loss"]
  File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/atuin/g102ea/g102ea22/mr-Audio/lavis/models/blip2_mr_models/blip2_mr.py", line 393, in forward
    outputs_loc = self.t5_model(
  File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1844, in _call_impl
    return inner()
  File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1790, in inner
    result = forward_call(*args, **kwargs)
  File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/peft/peft_model.py", line 1719, in forward
    return self.base_model(
  File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/peft/tuners/tuners_utils.py", line 197, in forward
    return self.model.forward(*args, **kwargs)
  File "/home/atuin/g102ea/g102ea22/mr-Audio/lavis/models/blip2_models/modeling_t5.py", line 1851, in forward
    encoder_outputs = self.encoder(
  File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/atuin/g102ea/g102ea22/mr-Audio/lavis/models/blip2_models/modeling_t5.py", line 1255, in forward
    layer_outputs = layer_module(
  File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/atuin/g102ea/g102ea22/mr-Audio/lavis/models/blip2_models/modeling_t5.py", line 851, in forward
    hidden_states = self.layer[-1](hidden_states)
  File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/atuin/g102ea/g102ea22/mr-Audio/lavis/models/blip2_models/modeling_t5.py", line 363, in forward
    forwarded_states = self.DenseReluDense(forwarded_states)
  File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/atuin/g102ea/g102ea22/mr-Audio/lavis/models/blip2_models/modeling_t5.py", line 343, in forward
    hidden_states = self.wo(hidden_states)
  File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/peft/tuners/lora/layer.py", line 621, in forward
    result = result + lora_B(lora_A(dropout(x))) * scaling
  File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/dropout.py", line 70, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/functional.py", line 1425, in dropout
    _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 15.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 37.75 GiB is allocated by PyTorch, and 241.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/atuin/g102ea/g102ea22/mr-Audio/train.py", line 154, in <module>
[rank0]:     main()
[rank0]:   File "/home/atuin/g102ea/g102ea22/mr-Audio/train.py", line 150, in main
[rank0]:     runner.train()
[rank0]:   File "/home/atuin/g102ea/g102ea22/mr-Audio/lavis/runners/runner_base.py", line 386, in train
[rank0]:     train_stats = self.train_epoch(cur_epoch)
[rank0]:   File "/home/atuin/g102ea/g102ea22/mr-Audio/lavis/runners/runner_base.py", line 445, in train_epoch
[rank0]:     return self.task.train_epoch(
[rank0]:   File "/home/atuin/g102ea/g102ea22/mr-Audio/lavis/tasks/base_task.py", line 116, in train_epoch
[rank0]:     return self._train_inner_loop(
[rank0]:   File "/home/atuin/g102ea/g102ea22/mr-Audio/lavis/tasks/moment_retrieval.py", line 242, in _train_inner_loop
[rank0]:     loss = self.train_step(model=model, samples=samples)
[rank0]:   File "/home/atuin/g102ea/g102ea22/mr-Audio/lavis/tasks/base_task.py", line 67, in train_step
[rank0]:     loss = model(samples)["loss"]
[rank0]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
[rank0]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank0]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
[rank0]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank0]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/atuin/g102ea/g102ea22/mr-Audio/lavis/models/blip2_mr_models/blip2_mr.py", line 393, in forward
[rank0]:     outputs_loc = self.t5_model(
[rank0]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1844, in _call_impl
[rank0]:     return inner()
[rank0]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1790, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/peft/peft_model.py", line 1719, in forward
[rank0]:     return self.base_model(
[rank0]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/peft/tuners/tuners_utils.py", line 197, in forward
[rank0]:     return self.model.forward(*args, **kwargs)
[rank0]:   File "/home/atuin/g102ea/g102ea22/mr-Audio/lavis/models/blip2_models/modeling_t5.py", line 1851, in forward
[rank0]:     encoder_outputs = self.encoder(
[rank0]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/atuin/g102ea/g102ea22/mr-Audio/lavis/models/blip2_models/modeling_t5.py", line 1255, in forward
[rank0]:     layer_outputs = layer_module(
[rank0]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/atuin/g102ea/g102ea22/mr-Audio/lavis/models/blip2_models/modeling_t5.py", line 851, in forward
[rank0]:     hidden_states = self.layer[-1](hidden_states)
[rank0]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/atuin/g102ea/g102ea22/mr-Audio/lavis/models/blip2_models/modeling_t5.py", line 363, in forward
[rank0]:     forwarded_states = self.DenseReluDense(forwarded_states)
[rank0]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/atuin/g102ea/g102ea22/mr-Audio/lavis/models/blip2_models/modeling_t5.py", line 343, in forward
[rank0]:     hidden_states = self.wo(hidden_states)
[rank0]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/peft/tuners/lora/layer.py", line 621, in forward
[rank0]:     result = result + lora_B(lora_A(dropout(x))) * scaling
[rank0]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/modules/dropout.py", line 70, in forward
[rank0]:     return F.dropout(input, self.p, self.training, self.inplace)
[rank0]:   File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/nn/functional.py", line 1425, in dropout
[rank0]:     _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 15.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 37.75 GiB is allocated by PyTorch, and 241.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
W1216 03:14:15.727160 1314283 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1314364 closing signal SIGTERM
W1216 03:14:15.733532 1314283 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1314365 closing signal SIGTERM
W1216 03:14:15.740634 1314283 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1314366 closing signal SIGTERM
E1216 03:14:16.158798 1314283 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 3 (pid: 1314367) of binary: /home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/bin/python
Traceback (most recent call last):
  File "/apps/python/3.9-anaconda/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/apps/python/3.9-anaconda/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/distributed/run.py", line 923, in <module>
    main()
  File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/atuin/g102ea/g102ea22/venvs/mrBlipAudio/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-12-16_03:14:15
  host      : a0603.nhr.fau.de
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 1314367)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
